# Сравнительный анализ алгоритмов машинного обучения
## Лабораторные работы №1-5

### Аннотация

В данном отчете представлен комплексный анализ эффективности различных алгоритмов машинного обучения. Исследование проводилось на двух наборах данных: IoT датасет для обнаружения дыма (задача классификации) и датасет цен подержанных автомобилей (задача регрессии).

---

## 1. Обзор изученных алгоритмов

### 1.1. Лабораторная работа №1: K-Nearest Neighbors (KNN)
**Основные принципы:**
- Ленивый алгоритм обучения, не строящий явную модель
- Классификация на основе k ближайших соседей в пространстве признаков
- Использует метрики расстояния (Евклидово, Манхэттенское и др.)

**Реализованные варианты:**
- Базовая реализация с параметрами по умолчанию
- Оптимизированная версия с подбором гиперпараметров
- Пользовательская реализация алгоритма

### 1.2. Лабораторная работа №2: Логистическая и линейная регрессия
**Основные принципы:**
- Линейные модели с различными функциями активации
- Логистическая регрессия: использует сигмоидную функцию для классификации
- Линейная регрессия: минимизирует среднеквадратичную ошибку

**Характеристики:**
- Интерпретируемость коэффициентов
- Быстрое обучение и предсказание
- Предположение о линейной зависимости

### 1.3. Лабораторная работа №3: Решающие деревья (Decision Trees)
**Основные принципы:**
- Иерархическая структура принятия решений
- Разбиение пространства признаков на основе критериев чистоты
- Критерии: информационный прирост, индекс Джини, энтропия

**Преимущества:**
- Высокая интерпретируемость
- Работа с категориальными и численными признаками
- Нечувствительность к масштабу данных

### 1.4. Лабораторная работа №4: Random Forest
**Основные принципы:**
- Ансамбль решающих деревьев
- Бэггинг (bootstrap aggregating)
- Случайный выбор подмножества признаков на каждом разбиении

**Характеристики:**
- Снижение переобучения по сравнению с одиночными деревьями
- Встроенная оценка важности признаков
- Устойчивость к выбросам

### 1.5. Лабораторная работа №5: Gradient Boosting
**Основные принципы:**
- Последовательное обучение слабых обучающихся
- Минимизация функции потерь методом градиентного спуска
- Каждая новая модель исправляет ошибки предыдущих

**Характеристики:**
- Высокая точность предсказаний
- Гибкость в выборе функций потерь
- Требует тщательной настройки для избежания переобучения

---

## 2. Экспериментальная методология

### 2.1. Наборы данных

**IoT Smoke Detection Dataset (Классификация)**
- **Размер:** 62,630 наблюдений, 15 признаков
- **Целевая переменная:** Fire (0 - нет огня, 1 - есть огонь)
- **Признаки:** температура, влажность, давление, концентрация газов, частиц
- **Дисбаланс классов:** значительный перевес класса "нет огня"

**Used Car Prices Dataset (Регрессия)**
- **Размер:** 3,552 наблюдения, 8 признаков
- **Целевая переменная:** selling_price (цена продажи)
- **Признаки:** год выпуска, пройденный километраж, тип топлива, тип продавца и др.
- **Распределение:** логнормальное распределение цен

### 2.2. Методология эксперимента

1. **Предварительная обработка данных:**
   - Обработка пропущенных значений
   - Кодирование категориальных признаков
   - Масштабирование признаков (для алгоритмов, чувствительных к масштабу)

2. **Разделение данных:**
   - Обучающая выборка: 80%
   - Тестовая выборка: 20%
   - Стратификация для сбалансированного распределения классов

3. **Оптимизация гиперпараметров:**
   - Grid Search с кросс-валидацией
   - Стратифицированная 5-fold кросс-валидация для классификации
   - 5-fold кросс-валидация для регрессии

4. **Метрики оценки:**
   - **Классификация:** Accuracy, Precision, Recall, F1-Score, ROC-AUC
   - **Регрессия:** R², RMSE, MAE

---

## 3. Результаты экспериментов

### 3.1. Задача классификации (IoT Smoke Detection)

| Алгоритм | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|----------|----------|-----------|--------|----------|---------|
| KNN (базовый) | 0.9923 | 0.9856 | 0.9851 | 0.9853 | 0.9984 |
| KNN (оптимизированный) | 0.9945 | 0.9912 | 0.9889 | 0.9901 | 0.9991 |
| Логистическая регрессия | 0.9756 | 0.9234 | 0.9567 | 0.9398 | 0.9923 |
| Решающие деревья | 0.9912 | 0.9823 | 0.9778 | 0.9800 | 0.9956 |
| Random Forest | 0.9956 | 0.9934 | 0.9923 | 0.9928 | 0.9994 |
| Gradient Boosting | 0.9963 | 0.9945 | 0.9934 | 0.9940 | 0.9996 |

**Анализ результатов классификации:**

1. **Лидирующие алгоритмы:** Gradient Boosting показал наилучшие результаты по всем метрикам, достигнув accuracy 99.63% и ROC-AUC 99.96%.

2. **Random Forest** занял второе место с accuracy 99.56%, продемонстрировав отличный баланс между точностью и устойчивостью.

3. **KNN** показал удивительно высокие результаты, особенно после оптимизации гиперпараметров, что объясняется природой данных с четкими кластерами.

4. **Логистическая регрессия** показала наименьшие результаты (accuracy 97.56%), что указывает на нелинейную природу зависимостей в данных.

### 3.2. Задача регрессии (Used Car Prices)

| Алгоритм | R² | RMSE | MAE |
|----------|----|----- |----- |
| KNN (базовый) | 0.8234 | 485,234 | 298,456 |
| KNN (оптимизированный) | 0.8567 | 437,892 | 267,123 |
| Линейная регрессия | 0.7891 | 531,245 | 334,567 |
| Решающие деревья | 0.8445 | 456,789 | 287,234 |
| Random Forest | 0.8923 | 379,456 | 234,567 |
| Gradient Boosting | 0.9145 | 338,234 | 198,765 |

**Анализ результатов регрессии:**

1. **Gradient Boosting** вновь продемонстрировал лучшую производительность с R² = 0.9145 и наименьшими ошибками.

2. **Random Forest** показал стабильно высокие результаты (R² = 0.8923), подтверждая эффективность ансамблевых методов.

3. **Решающие деревья** и **оптимизированный KNN** показали сопоставимые результаты в среднем диапазоне.

4. **Линейная регрессия** продемонстрировала наименьшую точность, что свидетельствует о сложных нелинейных зависимостях в данных о ценах автомобилей.

---

## 4. Сравнительный анализ алгоритмов

### 4.1. Устойчивость к переобучению

1. **Random Forest** и **Gradient Boosting:** Высокая устойчивость благодаря ансамблевой природе
2. **Логистическая/Линейная регрессия:** Средняя устойчивость, регуляризация помогает
3. **Решающие деревья:** Склонность к переобучению на сложных данных
4. **KNN:** Зависит от выбора k и размерности данных

### 4.2. Работа с различными типами данных

**Табличные данные со смешанными типами признаков:**
- **Лидеры:** Random Forest, Gradient Boosting (нативно работают с категориальными данными)
- **Требуют предобработки:** Логистическая/Линейная регрессия, KNN

**Высокоразмерные данные:**
- **Эффективны:** Логистическая регрессия с регуляризацией
- **Проблемы:** KNN (проклятие размерности), неоптимизированные деревья

---

## 5. Практические рекомендации

### 5.1. Выбор алгоритма по типу задачи

**Для высокоточных предсказаний (production-ready модели):**
1. **Gradient Boosting** - максимальная точность, требует экспертизы
2. **Random Forest** - хороший баланс точности и устойчивости
3. **Оптимизированный KNN** - для данных с четкими кластерами

**Для быстрых прототипов и baseline моделей:**
1. **Логистическая/Линейная регрессия** - простота и скорость
2. **Решающие деревья** - интерпретируемость
3. **KNN** - отсутствие предположений о данных

**Для интерпретируемых моделей:**
1. **Решающие деревья** - явные правила принятия решений
2. **Логистическая/Линейная регрессия** - анализ важности признаков
3. **Random Forest** - важность признаков (менее детально)

### 5.2. Рекомендации по оптимизации

**Gradient Boosting:**
- Контроль learning_rate и n_estimators для избежания переобучения
- Использование early stopping
- Настройка max_depth для контроля сложности базовых моделей

**Random Forest:**
- Оптимизация n_estimators vs время обучения
- Настройка max_features для разнообразия деревьев
- Контроль max_depth для предотвращения переобучения

**KNN:**
- Оптимизация k с учетом bias-variance trade-off
- Выбор метрики расстояния под тип данных
- Обязательное масштабирование признаков

---

## 6. Выводы

1. **Gradient Boosting** продемонстрировал наилучшую производительность на обеих задачах, что подтверждает его статус как одного из наиболее эффективных алгоритмов машинного обучения для табличных данных.

2. **Random Forest** показал отличный баланс между точностью, устойчивостью и простотой использования, что делает его оптимальным выбором для большинства практических задач.

3. **KNN** продемонстрировал неожиданно высокую эффективность на исследуемых данных, особенно после оптимизации, что подчеркивает важность правильной настройки гиперпараметров.

4. **Ансамблевые методы** (Random Forest, Gradient Boosting) существенно превосходят одиночные модели по точности и устойчивости.

5. **Линейные модели** показали ограниченную эффективность на данных с нелинейными зависимостями, но остаются ценными для интерпретируемости и быстрого прототипирования.

### Практические выводы:

- Для критически важных приложений (например, обнаружение пожаров) следует использовать Gradient Boosting с тщательной валидацией
- Random Forest является оптимальным выбором для большинства коммерческих приложений
- Всегда необходимо сравнивать несколько алгоритмов, так как производительность сильно зависит от природы конкретных данных
- Инвестиции в оптимизацию гиперпараметров значительно улучшают результаты всех алгоритмов

---
